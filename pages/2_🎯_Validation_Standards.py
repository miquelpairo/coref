"""
Standard Validation Tool
========================
Herramienta dedicada para validaci√≥n de est√°ndares √≥pticos NIR.
Verifica alineamiento espectral post-mantenimiento.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, List, Tuple
import sys
from pathlib import Path
from datetime import datetime

# A√±adir directorio ra√≠z al path
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from core.file_handlers import load_tsv_file, get_spectral_columns
from auth import check_password
from buchi_streamlit_theme import apply_buchi_styles
from config import DEFAULT_VALIDATION_THRESHOLDS, CRITICAL_REGIONS, OFFSET_LIMITS

apply_buchi_styles()

# Corregir estilos del sidebar para mejor contraste
st.markdown("""
<style>
    /* Sidebar general */
    [data-testid="stSidebar"] {
        background-color: #2c5f3f;
    }
    
    [data-testid="stSidebar"] * {
        color: white !important;
    }
    
    /* Headers y t√≠tulos */
    [data-testid="stSidebar"] h2,
    [data-testid="stSidebar"] h3 {
        color: white !important;
    }
    
    /* Labels - completamente invisibles, solo texto blanco */
    [data-testid="stSidebar"] label {
        color: white !important;
        font-weight: 500 !important;
        font-size: 14px !important;
        margin-bottom: 8px !important;
        display: block !important;
        background: none !important;
        border: none !important;
        padding: 0 !important;
        box-shadow: none !important;
    }
    
    /* Dividers */
    [data-testid="stSidebar"] hr {
        border-color: rgba(255, 255, 255, 0.2) !important;
        margin: 20px 0 !important;
    }
    
    /* Expander */
    [data-testid="stSidebar"] [data-testid="stExpander"] {
        background-color: rgba(255, 255, 255, 0.05) !important;
        border: 1px solid rgba(255, 255, 255, 0.1) !important;
        border-radius: 8px !important;
    }
    
    [data-testid="stSidebar"] [data-testid="stExpander"] summary {
        color: white !important;
    }
    
    /* Inputs - FONDO BLANCO con texto oscuro */
    [data-testid="stSidebar"] input[type="number"],
    [data-testid="stSidebar"] input[type="text"] {
        background-color: white !important;
        color: #333333 !important;
        border: 1px solid rgba(255, 255, 255, 0.3) !important;
        border-radius: 6px !important;
        padding: 8px 12px !important;
    }
    
    [data-testid="stSidebar"] input[type="number"]:focus,
    [data-testid="stSidebar"] input[type="text"]:focus {
        border-color: #7cb342 !important;
        box-shadow: 0 0 0 1px #7cb342 !important;
    }
    
    /* Number input - botones +/- GRISES OSCUROS sobre blanco */
    [data-testid="stSidebar"] [data-testid="stNumberInput"] button {
        background-color: #f0f0f0 !important;
        color: #333333 !important;
    }
    
    [data-testid="stSidebar"] [data-testid="stNumberInput"] button:hover {
        background-color: #e0e0e0 !important;
    }
    
    [data-testid="stSidebar"] [data-testid="stNumberInput"] button svg {
        color: #333333 !important;
        fill: #333333 !important;
    }
</style>
""", unsafe_allow_html=True)

if not check_password():
    st.stop()

# ============================================================================
# FUNCIONES DE VALIDACI√ìN
# ============================================================================

def validate_standard(reference: np.ndarray, current: np.ndarray, 
                     thresholds: Dict) -> Dict:
    """
    Valida un est√°ndar comparando medici√≥n actual vs referencia.
    
    Returns:
        Dict con m√©tricas y veredicto
    """
    # Asegurar que son float
    reference = np.asarray(reference, dtype=np.float64)
    current = np.asarray(current, dtype=np.float64)
    
    # 1. Correlaci√≥n espectral
    ref_norm = (reference - np.mean(reference)) / (np.std(reference) + 1e-10)
    curr_norm = (current - np.mean(current)) / (np.std(current) + 1e-10)
    correlation = np.sum(ref_norm * curr_norm) / len(ref_norm)
    
    # 2. Diferencias
    diff = current - reference
    max_diff = np.abs(diff).max()
    rms = np.sqrt(np.mean(diff**2))
    mean_diff = np.mean(diff)
    
    # 3. Evaluaci√≥n contra umbrales
    checks = {
        'correlation': correlation >= thresholds['correlation'],
        'max_diff': max_diff <= thresholds['max_diff'],
        'rms': rms <= thresholds['rms']
    }
    
    all_pass = all(checks.values())
    
    return {
        'correlation': correlation,
        'max_diff': max_diff,
        'rms': rms,
        'mean_diff': mean_diff,
        'checks': checks,
        'pass': all_pass,
        'diff': diff
    }


def detect_spectral_shift(reference: np.ndarray, current: np.ndarray, 
                         window: int = 5) -> Tuple[bool, float]:
    """
    Detecta si hay un shift sistem√°tico en longitud de onda.
    
    Returns:
        (tiene_shift, magnitud_promedio_shift)
    """
    # Calcular correlaci√≥n cruzada
    correlation = np.correlate(reference, current, mode='same')
    peak_pos = np.argmax(correlation)
    center = len(correlation) // 2
    
    shift = peak_pos - center
    
    # Si shift > window p√≠xeles, considerarlo significativo
    has_shift = abs(shift) > window
    
    return has_shift, float(shift)


def find_common_ids(df_ref: pd.DataFrame, df_curr: pd.DataFrame) -> pd.DataFrame:
    """
    Encuentra IDs comunes entre referencia y actual, emparejando solo por ID.
    Si hay m√∫ltiples filas con el mismo ID, toma la primera.
    
    Returns:
        DataFrame con columnas: ID, ref_note, curr_note, ref_idx, curr_idx
    """
    # Validar que los DataFrames no est√°n vac√≠os
    if len(df_ref) == 0 or len(df_curr) == 0:
        return pd.DataFrame(columns=['ID', 'ref_note', 'curr_note', 'ref_idx', 'curr_idx'])
    
    # Validar que tienen columna 'ID'
    if 'ID' not in df_ref.columns or 'ID' not in df_curr.columns:
        return pd.DataFrame(columns=['ID', 'ref_note', 'curr_note', 'ref_idx', 'curr_idx'])
    
    # Validar que tienen columna 'Note'
    if 'Note' not in df_ref.columns or 'Note' not in df_curr.columns:
        return pd.DataFrame(columns=['ID', 'ref_note', 'curr_note', 'ref_idx', 'curr_idx'])
    
    # Crear listas para almacenar los resultados
    ref_data = []
    for id_val in df_ref['ID'].unique():
        if pd.isna(id_val):  # Saltar IDs nulos
            continue
        mask = df_ref['ID'] == id_val
        indices = df_ref[mask].index
        if len(indices) > 0:
            first_idx = indices[0]
            ref_data.append({
                'ID': id_val,
                'ref_note': df_ref.loc[first_idx, 'Note'] if 'Note' in df_ref.columns else '',
                'ref_idx': first_idx
            })
    
    curr_data = []
    for id_val in df_curr['ID'].unique():
        if pd.isna(id_val):  # Saltar IDs nulos
            continue
        mask = df_curr['ID'] == id_val
        indices = df_curr[mask].index
        if len(indices) > 0:
            first_idx = indices[0]
            curr_data.append({
                'ID': id_val,
                'curr_note': df_curr.loc[first_idx, 'Note'] if 'Note' in df_curr.columns else '',
                'curr_idx': first_idx
            })
    
    # Validar que encontramos datos
    if len(ref_data) == 0 or len(curr_data) == 0:
        return pd.DataFrame(columns=['ID', 'ref_note', 'curr_note', 'ref_idx', 'curr_idx'])
    
    # Crear DataFrames
    df_ref_ids = pd.DataFrame(ref_data)
    df_curr_ids = pd.DataFrame(curr_data)
    
    # Hacer merge solo por ID
    matches = df_ref_ids.merge(df_curr_ids, on='ID', how='inner')
    
    return matches[['ID', 'ref_note', 'curr_note', 'ref_idx', 'curr_idx']]


def analyze_critical_regions(reference: np.ndarray, current: np.ndarray,
                            regions: List[Tuple[int, int]], 
                            num_channels: int) -> pd.DataFrame:
    """
    Analiza diferencias en regiones espectrales cr√≠ticas.
    Asume rango 900-1700 nm para 256 p√≠xeles.
    """
    wavelength_per_pixel = 800 / num_channels  # (1700-900)/256
    start_wl = 900
    end_wl = 1700
    
    results = []
    
    for wl_start, wl_end in regions:
        # Verificar si la regi√≥n est√° dentro del rango del instrumento
        if wl_end < start_wl or wl_start > end_wl:
            results.append({
                'Regi√≥n (nm)': f"{wl_start}-{wl_end}",
                'Canales': "Fuera de rango",
                'Max |Œî|': "N/A",
                'RMS': "N/A",
                'Media Œî': "N/A"
            })
            continue
        
        # Ajustar regi√≥n a los l√≠mites del instrumento
        wl_start_adjusted = max(wl_start, start_wl)
        wl_end_adjusted = min(wl_end, end_wl)
        
        # Convertir wavelength a √≠ndices de p√≠xel
        px_start = int((wl_start_adjusted - start_wl) / wavelength_per_pixel)
        px_end = int((wl_end_adjusted - start_wl) / wavelength_per_pixel)
        
        px_start = max(0, px_start)
        px_end = min(num_channels, px_end)
        
        # Verificar que hay al menos algunos p√≠xeles en la regi√≥n
        if px_end <= px_start:
            results.append({
                'Regi√≥n (nm)': f"{wl_start}-{wl_end}",
                'Canales': "Regi√≥n muy peque√±a",
                'Max |Œî|': "N/A",
                'RMS': "N/A",
                'Media Œî': "N/A"
            })
            continue
        
        # Extraer regi√≥n
        ref_region = reference[px_start:px_end]
        curr_region = current[px_start:px_end]
        
        # Calcular m√©tricas
        diff_region = curr_region - ref_region
        
        region_label = f"{wl_start}-{wl_end}"
        if wl_start_adjusted != wl_start or wl_end_adjusted != wl_end:
            region_label += f" *"  # Asterisco si fue ajustado
        
        results.append({
            'Regi√≥n (nm)': region_label,
            'Canales': f"{px_start}-{px_end}",
            'Max |Œî|': f"{np.abs(diff_region).max():.6f}",
            'RMS': f"{np.sqrt(np.mean(diff_region**2)):.6f}",
            'Media Œî': f"{np.mean(diff_region):.6f}"
        })
    
    return pd.DataFrame(results)


# ============================================================================
# VISUALIZACIONES
# ============================================================================

def create_validation_plot(reference: np.ndarray, current: np.ndarray,
                          diff: np.ndarray, sample_label: str) -> go.Figure:
    """Crea gr√°fico de 3 paneles para validaci√≥n."""
    
    channels = list(range(1, len(reference) + 1))
    
    fig = make_subplots(
        rows=3, cols=1,
        subplot_titles=(
            f'Espectros: Referencia vs Actual ({sample_label})',
            'Diferencia (Actual - Referencia)',
            'Diferencia Acumulada'
        ),
        vertical_spacing=0.1,
        row_heights=[0.4, 0.3, 0.3]
    )
    
    # Panel 1: Overlay
    fig.add_trace(
        go.Scatter(x=channels, y=reference, name='Referencia',
                  line=dict(color='blue', width=2)),
        row=1, col=1
    )
    fig.add_trace(
        go.Scatter(x=channels, y=current, name='Actual',
                  line=dict(color='red', width=2, dash='dash')),
        row=1, col=1
    )
    
    # Panel 2: Diferencia
    fig.add_trace(
        go.Scatter(x=channels, y=diff, name='Œî',
                  line=dict(color='green', width=2),
                  fill='tozeroy', fillcolor='rgba(0,255,0,0.1)'),
        row=2, col=1
    )
    fig.add_hline(y=0, line_dash="dash", line_color="gray", 
                  opacity=0.5, row=2, col=1)
    
    # Panel 3: Diferencia acumulada
    cumsum_diff = np.cumsum(diff)
    fig.add_trace(
        go.Scatter(x=channels, y=cumsum_diff, name='Œ£ Œî',
                  line=dict(color='purple', width=2)),
        row=3, col=1
    )
    fig.add_hline(y=0, line_dash="dash", line_color="gray", 
                  opacity=0.5, row=3, col=1)
    
    fig.update_xaxes(title_text="Canal espectral", row=3, col=1)
    fig.update_yaxes(title_text="Absorbancia", row=1, col=1)
    fig.update_yaxes(title_text="Œî Absorbancia", row=2, col=1)
    fig.update_yaxes(title_text="Œ£ Œî", row=3, col=1)
    
    fig.update_layout(
        height=900,
        showlegend=True,
        template='plotly_white',
        hovermode='x unified'
    )
    
    return fig


def create_overlay_plot(validation_data: List[Dict], show_reference: bool = True,
                       show_current: bool = True) -> go.Figure:
    """
    Crea gr√°fico con overlay de todos los espectros de validaci√≥n.
    
    Args:
        validation_data: Lista de diccionarios con datos de validaci√≥n
        show_reference: Si True, muestra espectros de referencia
        show_current: Si True, muestra espectros actuales
    """
    colors_ref = ['#1f77b4', '#2ca02c', '#9467bd', '#8c564b', '#e377c2', 
                  '#7f7f7f', '#bcbd22', '#17becf', '#ff9896', '#c5b0d5']
    colors_curr = ['#ff7f0e', '#d62728', '#ff69b4', '#ffa500', '#dc143c',
                   '#ff4500', '#ff1493', '#ff6347', '#ff8c00', '#ff00ff']
    
    fig = go.Figure()
    
    if len(validation_data) == 0:
        return fig
    
    channels = list(range(1, len(validation_data[0]['reference']) + 1))
    
    # A√±adir espectros de referencia
    if show_reference:
        for i, data in enumerate(validation_data):
            color = colors_ref[i % len(colors_ref)]
            sample_label = f"{data['id']} - Ref"
            
            fig.add_trace(go.Scatter(
                x=channels,
                y=data['reference'],
                mode='lines',
                name=sample_label,
                line=dict(color=color, width=2),
                legendgroup='reference',
                hovertemplate=f'<b>{sample_label}</b><br>' +
                             'Canal: %{x}<br>' +
                             'Absorbancia: %{y:.6f}<br>' +
                             '<extra></extra>'
            ))
    
    # A√±adir espectros actuales
    if show_current:
        for i, data in enumerate(validation_data):
            color = colors_curr[i % len(colors_curr)]
            sample_label = f"{data['id']} - Act"
            
            fig.add_trace(go.Scatter(
                x=channels,
                y=data['current'],
                mode='lines',
                name=sample_label,
                line=dict(color=color, width=2, dash='dash'),
                legendgroup='current',
                hovertemplate=f'<b>{sample_label}</b><br>' +
                             'Canal: %{x}<br>' +
                             'Absorbancia: %{y:.6f}<br>' +
                             '<extra></extra>'
            ))
    
    fig.update_layout(
        title={
            'text': 'Comparaci√≥n Global de Todos los Est√°ndares',
            'x': 0.5,
            'xanchor': 'center',
            'font': {'size': 20, 'color': '#2c5f3f'}
        },
        xaxis_title='Canal espectral',
        yaxis_title='Absorbancia',
        hovermode='closest',
        template='plotly_white',
        height=600,
        showlegend=True,
        legend=dict(
            orientation="v",
            yanchor="top",
            y=1,
            xanchor="left",
            x=1.02,
            font=dict(size=10)
        )
    )
    
    return fig


def create_global_statistics_table(validation_data: List[Dict]) -> pd.DataFrame:
    """
    Crea tabla con estad√≠sticas globales de todos los est√°ndares.
    """
    if len(validation_data) == 0:
        return pd.DataFrame()
    
    # Recopilar m√©tricas de todos los est√°ndares
    all_correlations = []
    all_max_diffs = []
    all_rms = []
    all_mean_diffs = []
    
    for data in validation_data:
        val_res = data['validation_results']
        all_correlations.append(val_res['correlation'])
        all_max_diffs.append(val_res['max_diff'])
        all_rms.append(val_res['rms'])
        all_mean_diffs.append(val_res['mean_diff'])
    
    # Calcular estad√≠sticas
    stats = {
        'M√©trica': ['Correlaci√≥n', 'Max Diferencia (AU)', 'RMS', 'Offset Medio (AU)'],
        'M√≠nimo': [
            f"{min(all_correlations):.6f}",
            f"{min(all_max_diffs):.6f}",
            f"{min(all_rms):.6f}",
            f"{min(all_mean_diffs):.6f}"
        ],
        'M√°ximo': [
            f"{max(all_correlations):.6f}",
            f"{max(all_max_diffs):.6f}",
            f"{max(all_rms):.6f}",
            f"{max(all_mean_diffs):.6f}"
        ],
        'Media': [
            f"{np.mean(all_correlations):.6f}",
            f"{np.mean(all_max_diffs):.6f}",
            f"{np.mean(all_rms):.6f}",
            f"{np.mean(all_mean_diffs):.6f}"
        ],
        'Desv. Est.': [
            f"{np.std(all_correlations):.6f}",
            f"{np.std(all_max_diffs):.6f}",
            f"{np.std(all_rms):.6f}",
            f"{np.std(all_mean_diffs):.6f}"
        ]
    }
    
    return pd.DataFrame(stats)


# ============================================================================
# INTERFAZ PRINCIPAL
# ============================================================================

def main():
    st.title("üéØ Standard Validation Tool")
    st.markdown("**Validaci√≥n autom√°tica de est√°ndares √≥pticos post-mantenimiento**")
    
    # Info inicial
    st.info("""
    Esta herramienta valida que el alineamiento espectral del equipo se mantiene 
    correcto despu√©s de realizar mantenimiento (ej: cambio de l√°mpara).
    
    **Proceso:**
    1. Carga archivos TSV con mediciones antes y despu√©s del mantenimiento
    2. Selecciona los est√°ndares a validar
    3. Analiza autom√°ticamente la correlaci√≥n y diferencias espectrales
    4. Genera informe detallado con resultados
    """)
    
    st.divider()
    
    # Sidebar
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è Configuraci√≥n")
        st.markdown("---")
        
        # Ajuste de umbrales
        with st.expander("üéöÔ∏è Ajustar Umbrales de Validaci√≥n"):
            st.caption("Modifica los criterios de aceptaci√≥n seg√∫n tus necesidades")
            
            corr_threshold = st.number_input(
                "Correlaci√≥n m√≠nima:",
                min_value=0.990,
                max_value=1.000,
                value=DEFAULT_VALIDATION_THRESHOLDS['correlation'],
                step=0.001,
                format="%.3f",
                help="Similitud espectral m√≠nima aceptable (valores cercanos a 1.0 = alta similitud)"
            )
            
            max_diff_threshold = st.number_input(
                "Diferencia m√°xima (AU):",
                min_value=0.001,
                max_value=0.100,
                value=DEFAULT_VALIDATION_THRESHOLDS['max_diff'],
                step=0.001,
                format="%.3f",
                help="M√°xima desviaci√≥n puntual permitida en absorbancia"
            )
            
            rms_threshold = st.number_input(
                "RMS m√°ximo:",
                min_value=0.001,
                max_value=0.100,
                value=DEFAULT_VALIDATION_THRESHOLDS['rms'],
                step=0.001,
                format="%.3f",
                help="Error cuadr√°tico medio m√°ximo aceptable"
            )
            
            thresholds = {
                'correlation': corr_threshold,
                'max_diff': max_diff_threshold,
                'rms': rms_threshold
            }
        
        if 'thresholds' not in locals():
            thresholds = DEFAULT_VALIDATION_THRESHOLDS
        
        st.divider()
        
        # Info de regiones cr√≠ticas
        with st.expander("‚ÑπÔ∏è Regiones Espectrales Cr√≠ticas"):
            st.markdown("""
            **Regiones analizadas:**
            - **1100-1200 nm**: Enlaces O-H (hidroxilos)
            - **1400-1500 nm**: Agua / Humedad
            - **1600-1700 nm**: Enlaces C-H (grupos metilo)
            
            Estas regiones son especialmente sensibles a desalineamientos √≥pticos.
            """)
    
    # ==========================================
    # SECCI√ìN: CARGA DE ARCHIVOS TSV
    # ==========================================
    st.markdown("### üìÅ Carga de Archivos de Medici√≥n")

    st.info("""
    Carga dos archivos TSV con mediciones de est√°ndares √≥pticos:

    **Uso t√≠pico:**
    - **Referencia**: Mediciones con baseline antigua (antes de mantenimiento)
    - **Actual**: Mediciones con baseline nueva (despu√©s de mantenimiento)
    """)

    col1, col2 = st.columns(2)

    with col1:
        ref_file = st.file_uploader(
            "Referencia pre-mantenimiento:",
            type=['tsv'],
            key="ref_tsv_validation",
            help="Archivo TSV con mediciones de est√°ndares ANTES del mantenimiento (con baseline antigua)"
        )

    with col2:
        curr_file = st.file_uploader(
            "Medici√≥n post-mantenimiento:",
            type=['tsv'],
            key="curr_tsv_validation",
            help="Archivo TSV con mediciones de est√°ndares DESPU√âS del mantenimiento (con baseline nueva)"
        )

    st.divider()
    
    # √Årea principal
    if not ref_file or not curr_file:
        st.info("üëÜ Carga ambos archivos para comenzar")
        
        with st.expander("üìñ Gu√≠a de Uso", expanded=True):
            st.markdown("""
            ### ¬øC√≥mo funciona esta herramienta?
            
            **1. Preparaci√≥n**
            - Realiza mediciones de est√°ndares √≥pticos ANTES del mantenimiento
            - Exporta las mediciones como archivo TSV (Referencia)
            
            **2. Mantenimiento**
            - Realiza el cambio de l√°mpara u otro mantenimiento
            - Instala el nuevo baseline corregido
            
            **3. Verificaci√≥n**
            - Mide los mismos est√°ndares con el nuevo baseline
            - Exporta las mediciones como archivo TSV (Post-mantenimiento)
            
            **4. Validaci√≥n Autom√°tica**
            - Esta herramienta compara ambos conjuntos de mediciones
            - Detecta desviaciones, correlaciones y shifts espectrales
            - Genera un informe detallado
            
            ---
            
            **Umbrales por defecto:**
            - Correlaci√≥n espectral: ‚â• 0.999
            - Diferencia m√°xima: ‚â§ 0.02 AU
            - RMS: ‚â§ 0.015
            
            Estos valores se pueden ajustar en el panel lateral seg√∫n tus requisitos.
            """)
        
        return
    
    # Cargar archivos
    try:
        with st.spinner("‚è≥ Cargando archivos y detectando est√°ndares comunes..."):
            df_ref = load_tsv_file(ref_file)
            df_curr = load_tsv_file(curr_file)
            
            spectral_cols_ref = get_spectral_columns(df_ref)
            spectral_cols_curr = get_spectral_columns(df_curr)
            
            if len(spectral_cols_ref) != len(spectral_cols_curr):
                st.error("‚ùå Los archivos tienen diferente n√∫mero de canales espectrales")
                st.info(f"""
                - Archivo referencia: {len(spectral_cols_ref)} canales
                - Archivo actual: {len(spectral_cols_curr)} canales
                
                Ambos archivos deben tener el mismo n√∫mero de canales espectrales.
                """)
                return
            
            num_channels = len(spectral_cols_ref)
            
            # Encontrar IDs comunes
            matches = find_common_ids(df_ref, df_curr)
            
            if len(matches) == 0:
                st.error("‚ùå No se encontraron IDs comunes entre los archivos")
                st.info("üí° Verifica que:")
                st.markdown("""
                - Los campos **ID** y **Note** existen en ambos archivos
                - Los IDs utilizados son consistentes entre mediciones
                - Ambas mediciones incluyen los mismos est√°ndares
                """)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("#### IDs en Referencia")
                    if 'ID' in df_ref.columns and 'Note' in df_ref.columns:
                        st.dataframe(df_ref[['ID', 'Note']], use_container_width=True)
                    else:
                        st.warning("Columnas ID o Note no encontradas")
                        
                with col2:
                    st.markdown("#### IDs en Actual")
                    if 'ID' in df_curr.columns and 'Note' in df_curr.columns:
                        st.dataframe(df_curr[['ID', 'Note']], use_container_width=True)
                    else:
                        st.warning("Columnas ID o Note no encontradas")
                
                return
            
        st.success(f"‚úÖ {len(matches)} est√°ndar(es) com√∫n(es) detectado(s)")
            
    except Exception as e:
        st.error(f"‚ùå Error al cargar archivos: {str(e)}")
        with st.expander("üîç Ver detalles del error"):
            import traceback
            st.code(traceback.format_exc())
        return
    
    st.divider()
    
    # ==========================================
    # SECCI√ìN 1: SELECCI√ìN DE EST√ÅNDARES
    # ==========================================
    st.markdown("### 1Ô∏è‚É£ Selecci√≥n de Est√°ndares a Validar")
    
    # Inicializar estado de selecci√≥n
    common_ids = matches['ID'].tolist()

    if 'standards_selected_ids' not in st.session_state:
        st.session_state.standards_selected_ids = common_ids.copy()

    if 'standards_pending_selection' not in st.session_state:
        st.session_state.standards_pending_selection = common_ids.copy()

    # Crear DataFrame para mostrar
    df_samples = pd.DataFrame({
        'ID': common_ids,
        'Note (Ref)': matches['ref_note'].tolist(),
        'Note (Actual)': matches['curr_note'].tolist(),
        'Usar para validaci√≥n': [
            id_ in st.session_state.standards_pending_selection 
            for id_ in common_ids
        ]
    })

    st.info("""
    Selecciona los est√°ndares que deseas incluir en la validaci√≥n. 
    Por defecto, todos los est√°ndares comunes est√°n seleccionados.
    """)

    # Usar formulario
    with st.form("form_select_standards_validation", clear_on_submit=False):
        edited = st.data_editor(
            df_samples,
            use_container_width=True,
            hide_index=True,
            disabled=['ID', 'Note (Ref)', 'Note (Actual)'],
            column_config={
                "Usar para validaci√≥n": st.column_config.CheckboxColumn(
                    "‚úì Incluir",
                    help="Marcar para incluir en la validaci√≥n",
                    default=True,
                )
            },
            key="editor_select_standards_validation"
        )
        
        # Botones dentro del formulario
        col_a, col_b, col_c, col_d = st.columns(4)
        btn_all = col_a.form_submit_button("‚úÖ Todos", use_container_width=True)
        btn_none = col_b.form_submit_button("‚ùå Ninguno", use_container_width=True)
        btn_invert = col_c.form_submit_button("üîÑ Invertir", use_container_width=True)
        btn_confirm = col_d.form_submit_button("üöÄ Confirmar Selecci√≥n", type="primary", use_container_width=True)

    # Manejar acciones de botones
    if btn_all:
        st.session_state.standards_pending_selection = common_ids.copy()
        st.rerun()

    if btn_none:
        st.session_state.standards_pending_selection = []
        st.rerun()

    if btn_invert:
        inverted = [id_ for id_ in common_ids if id_ not in st.session_state.standards_pending_selection]
        st.session_state.standards_pending_selection = inverted
        st.rerun()

    if btn_confirm:
        try:
            pending = edited.loc[edited['Usar para validaci√≥n'], 'ID'].tolist()
            st.session_state.standards_pending_selection = pending
            st.session_state.standards_selected_ids = pending
            st.session_state['validated'] = True
            st.rerun()
        except Exception as e:
            st.error(f"Error al confirmar selecci√≥n: {str(e)}")
    else:
        # Actualizar pending mientras se edita
        if isinstance(edited, pd.DataFrame):
            try:
                pending = edited.loc[edited['Usar para validaci√≥n'], 'ID'].tolist()
                st.session_state.standards_pending_selection = pending
            except Exception:
                pass

    # Mostrar contador
    st.caption(
        f"Pendientes: {len(st.session_state.standards_pending_selection)} - "
        f"Confirmados: {len(st.session_state.get('standards_selected_ids', []))}"
    )

    # Solo proceder si ya se valid√≥
    if 'validated' not in st.session_state or not st.session_state['validated']:
        st.info("üëÜ Ajusta la selecci√≥n y presiona **'Confirmar Selecci√≥n'** para continuar")
        return

    # Recuperar selecci√≥n guardada
    selected_ids = st.session_state.standards_selected_ids
    matches_filtered = matches[matches['ID'].isin(selected_ids)].copy()

    st.divider()

    # Bot√≥n para volver a la selecci√≥n
    if st.button("üîÑ Cambiar Selecci√≥n de Est√°ndares", use_container_width=False):
        st.session_state['validated'] = False
        st.rerun()
    
    # ==========================================
    # SECCI√ìN 2: VALIDACI√ìN AUTOM√ÅTICA
    # ==========================================
    st.markdown("### 2Ô∏è‚É£ Resultados de Validaci√≥n")
    
    all_results = []
    all_validation_data = []
    
    with st.spinner(f"‚è≥ Validando {len(matches_filtered)} est√°ndar(es)..."):
        for idx, row in matches_filtered.iterrows():
            sample_id = row['ID']
            ref_note = row['ref_note']
            curr_note = row['curr_note']
            ref_idx = row['ref_idx']
            curr_idx = row['curr_idx']
            
            # Extraer espectros
            reference = df_ref.loc[ref_idx, spectral_cols_ref].astype(float).values
            current = df_curr.loc[curr_idx, spectral_cols_curr].astype(float).values
            
            # Validar
            validation_results = validate_standard(reference, current, thresholds)
            has_shift, shift_magnitude = detect_spectral_shift(reference, current)
            
            # Determinar estado
            if validation_results['pass'] and not has_shift:
                estado = "‚úÖ OK"
                estado_sort = 0
            elif validation_results['pass'] and has_shift:
                estado = "‚ö†Ô∏è Revisar"
                estado_sort = 1
            else:
                estado = "‚ùå Fallo"
                estado_sort = 2
            
            all_results.append({
                'Estado': estado,
                '_sort': estado_sort,
                'ID': sample_id,
                'Note (Ref)': ref_note,
                'Note (Actual)': curr_note,
                'Correlaci√≥n': f"{validation_results['correlation']:.6f}",
                'Max Œî (AU)': f"{validation_results['max_diff']:.6f}",
                'RMS': f"{validation_results['rms']:.6f}",
                'Shift (px)': f"{shift_magnitude:.1f}" if has_shift else "0.0"
            })
            
            # Guardar datos completos
            all_validation_data.append({
                'id': sample_id,
                'ref_note': ref_note,
                'curr_note': curr_note,
                'reference': reference,
                'current': current,
                'diff': validation_results['diff'],
                'validation_results': validation_results,
                'has_shift': has_shift,
                'shift_magnitude': shift_magnitude
            })
    
    # Crear DataFrame de resultados
    results_df = pd.DataFrame(all_results)
    results_df = results_df.sort_values('_sort').drop('_sort', axis=1)
    
    # Resumen general
    n_ok = sum(1 for r in all_results if r['Estado'] == "‚úÖ OK")
    n_warn = sum(1 for r in all_results if r['Estado'] == "‚ö†Ô∏è Revisar")
    n_fail = sum(1 for r in all_results if r['Estado'] == "‚ùå Fallo")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Est√°ndares", len(matches_filtered))
    with col2:
        st.metric("‚úÖ Validados", n_ok)
    with col3:
        st.metric("‚ö†Ô∏è Revisar", n_warn)
    with col4:
        st.metric("‚ùå Fallidos", n_fail)
    
    st.markdown("---")
    
    # Tabla resumen
    st.markdown("#### üìã Tabla Resumen")
    st.dataframe(results_df, use_container_width=True, hide_index=True)
    
    # Exportar resumen
    csv = results_df.to_csv(index=False)
    st.download_button(
        label="üì• Descargar Resumen (CSV)",
        data=csv,
        file_name=f"validation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv",
        use_container_width=True
    )
    
    st.divider()
    
    # ==========================================
    # SECCI√ìN 3: AN√ÅLISIS GLOBAL
    # ==========================================
    st.markdown("### 3Ô∏è‚É£ An√°lisis Global")
    
    # Expandable para gr√°fico overlay
    with st.expander("üìä Vista Global de Todos los Est√°ndares", expanded=False):
        st.info("""
        Comparaci√≥n simult√°nea de todos los espectros validados. 
        Las l√≠neas s√≥lidas representan las mediciones de referencia (pre-mantenimiento) 
        y las l√≠neas punteadas las mediciones actuales (post-mantenimiento).
        """)
        
        col1, col2 = st.columns([3, 1])
        
        with col2:
            show_ref = st.checkbox("Mostrar Referencia", value=True, key="show_ref_overlay")
            show_curr = st.checkbox("Mostrar Actual", value=True, key="show_curr_overlay")
        
        overlay_fig = create_overlay_plot(all_validation_data, show_ref, show_curr)
        st.plotly_chart(overlay_fig, use_container_width=True)
    
    # Estad√≠sticas globales
    st.markdown("#### üìà Estad√≠sticas Globales del Kit")
    st.caption(f"An√°lisis agregado de {len(all_validation_data)} est√°ndar(es)")
    
    stats_df = create_global_statistics_table(all_validation_data)
    st.dataframe(stats_df, use_container_width=True, hide_index=True)
    
    # M√©tricas destacadas
    col1, col2, col3 = st.columns(3)
    with col1:
        avg_corr = np.mean([d['validation_results']['correlation'] for d in all_validation_data])
        st.metric(
            "Correlaci√≥n Media", 
            f"{avg_corr:.6f}", 
            delta="OK" if avg_corr >= thresholds['correlation'] else "Revisar",
            delta_color="normal" if avg_corr >= thresholds['correlation'] else "inverse"
        )
    with col2:
        avg_max_diff = np.mean([d['validation_results']['max_diff'] for d in all_validation_data])
        st.metric(
            "Max Œî Media", 
            f"{avg_max_diff:.6f} AU",
            delta="OK" if avg_max_diff <= thresholds['max_diff'] else "Revisar",
            delta_color="normal" if avg_max_diff <= thresholds['max_diff'] else "inverse"
        )
    with col3:
        avg_rms = np.mean([d['validation_results']['rms'] for d in all_validation_data])
        st.metric(
            "RMS Media", 
            f"{avg_rms:.6f}",
            delta="OK" if avg_rms <= thresholds['rms'] else "Revisar",
            delta_color="normal" if avg_rms <= thresholds['rms'] else "inverse"
        )
    
    st.markdown("---")
    
    # Offset global del kit
    global_offset = np.mean([d['validation_results']['mean_diff'] for d in all_validation_data])
    
    st.metric(
        "üéØ Offset Global del Kit", 
        f"{global_offset:.6f} AU",
        help="Desplazamiento sistem√°tico promedio entre mediciones pre y post-mantenimiento"
    )
    
    if abs(global_offset) < OFFSET_LIMITS['negligible']:
        st.success("‚úÖ Offset despreciable - Excelente alineamiento")
    elif abs(global_offset) < OFFSET_LIMITS['acceptable']:
        st.info("‚ÑπÔ∏è Offset peque√±o - Alineamiento aceptable")
    else:
        st.warning(f"‚ö†Ô∏è Offset significativo detectado ({'+' if global_offset > 0 else ''}{global_offset:.6f} AU)")
    
    st.divider()
    
    # ==========================================
    # SECCI√ìN 4: AN√ÅLISIS INDIVIDUAL
    # ==========================================
    st.markdown("### 4Ô∏è‚É£ An√°lisis Detallado por Est√°ndar")
    st.info("An√°lisis individual de cada est√°ndar con gr√°ficos comparativos, diferencias espectrales y regiones cr√≠ticas.")

    # Filtro de b√∫squeda
    search_filter = st.text_input(
        "üîç Buscar est√°ndar por ID:",
        placeholder="Escribe para filtrar...",
        help="Filtra la lista de est√°ndares por ID"
    )

    # Filtrar lista seg√∫n b√∫squeda
    if search_filter:
        filtered_indices = [
            i for i in range(len(all_validation_data))
            if search_filter.lower() in str(all_validation_data[i]['id']).lower()
        ]
    else:
        filtered_indices = list(range(len(all_validation_data)))

    if len(filtered_indices) == 0:
        st.warning("‚ö†Ô∏è No se encontraron est√°ndares que coincidan con la b√∫squeda")
        return

    # Mostrar cu√°ntos resultados
    if search_filter:
        st.caption(f"Mostrando {len(filtered_indices)} de {len(all_validation_data)} est√°ndares")

    # Selector con lista filtrada
    selected_sample_filtered = st.selectbox(
        "Selecciona est√°ndar:",
        filtered_indices,
        format_func=lambda x: f"{all_validation_data[x]['id']} - {all_validation_data[x]['ref_note']}",
        key="sample_selector"
    )

    sample_data = all_validation_data[selected_sample_filtered]
    
    # Tabs de an√°lisis detallado
    tab1, tab2, tab3 = st.tabs([
        "üìà Gr√°ficos",
        "üìã Regiones Cr√≠ticas",
        "üìÑ M√©tricas"
    ])
    
    with tab1:
        sample_label = f"{sample_data['id']} (Ref: {sample_data['ref_note']} | Act: {sample_data['curr_note']})"
        fig = create_validation_plot(
            sample_data['reference'],
            sample_data['current'],
            sample_data['diff'],
            sample_label
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        regions_df = analyze_critical_regions(
            sample_data['reference'],
            sample_data['current'],
            CRITICAL_REGIONS,
            num_channels
        )
        st.dataframe(regions_df, use_container_width=True, hide_index=True)
        st.caption("* = Regi√≥n ajustada a rango del instrumento (900-1700 nm)")
    
    with tab3:
        val_res = sample_data['validation_results']
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### M√©tricas Calculadas")
            metrics_data = {
                'M√©trica': ['Correlaci√≥n', 'Max Diferencia', 'RMS', 'Media Œî', 'Shift Espectral'],
                'Valor': [
                    f"{val_res['correlation']:.6f}",
                    f"{val_res['max_diff']:.6f} AU",
                    f"{val_res['rms']:.6f}",
                    f"{val_res['mean_diff']:.6f}",
                    f"{sample_data['shift_magnitude']:.1f} px" if sample_data['has_shift'] else "No detectado"
                ]
            }
            st.dataframe(pd.DataFrame(metrics_data), use_container_width=True, hide_index=True)
        
        with col2:
            st.markdown("#### Evaluaci√≥n")
            checks_data = {
                'Criterio': ['Correlaci√≥n', 'Diferencia M√°xima', 'RMS'],
                'Umbral': [
                    f"‚â• {thresholds['correlation']}",
                    f"‚â§ {thresholds['max_diff']} AU",
                    f"‚â§ {thresholds['rms']}"
                ],
                'Estado': [
                    "‚úÖ OK" if val_res['checks']['correlation'] else "‚ùå Fallo",
                    "‚úÖ OK" if val_res['checks']['max_diff'] else "‚ùå Fallo",
                    "‚úÖ OK" if val_res['checks']['rms'] else "‚ùå Fallo"
                ]
            }
            st.dataframe(pd.DataFrame(checks_data), use_container_width=True, hide_index=True)
    
    st.divider()
    
    # ==========================================
    # SECCI√ìN 5: GENERACI√ìN DE INFORME
    # ==========================================
    st.markdown("### 5Ô∏è‚É£ Generar Informe de Validaci√≥n")
    st.info("""
    Completa la informaci√≥n del servicio para generar un informe HTML profesional 
    con todos los resultados de la validaci√≥n.
    """)
    
    st.markdown("#### üìã Informaci√≥n del Servicio")
    
    col1, col2 = st.columns(2)
    
    with col1:
        sensor_serial = st.text_input(
            "N√∫mero de Serie del Sensor:",
            placeholder="Ej: NIR-2024-001",
            help="N√∫mero de serie √∫nico del equipo NIR"
        )
        
        customer_name = st.text_input(
            "Cliente:",
            placeholder="Ej: Universidad de Barcelona",
            help="Nombre del cliente o instituci√≥n"
        )
    
    with col2:
        technician_name = st.text_input(
            "T√©cnico Responsable:",
            placeholder="Ej: Juan P√©rez",
            help="Nombre del t√©cnico que realiz√≥ el servicio de mantenimiento"
        )
        
        service_notes = st.text_area(
            "Notas del Servicio:",
            placeholder="Ej: Cambio de l√°mpara hal√≥gena, ajuste √≥ptico, limpieza de ventana...",
            help="Observaciones relevantes del mantenimiento realizado",
            height=100
        )
    
    st.markdown("---")
    
    # Bot√≥n de generaci√≥n centrado
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        if st.button("üì• Generar Informe HTML", type="primary", use_container_width=True):
            if not sensor_serial or not customer_name or not technician_name:
                st.error("‚ùå Por favor completa los campos obligatorios: N√∫mero de Serie, Cliente y T√©cnico")
            else:
                with st.spinner("‚è≥ Generando informe completo..."):
                    try:
                        from core.validation_kit_report_generator import generate_validation_report
                        
                        # Preparar datos para el reporte
                        report_data = {
                            'sensor_serial': sensor_serial,
                            'customer_name': customer_name,
                            'technician_name': technician_name,
                            'service_notes': service_notes,
                            'validation_data': all_validation_data,
                            'results_df': results_df,
                            'thresholds': thresholds,
                            'n_ok': n_ok,
                            'n_warn': n_warn,
                            'n_fail': n_fail,
                            'num_channels': num_channels,
                            'ref_filename': ref_file.name,
                            'curr_filename': curr_file.name
                        }
                        
                        html_content = generate_validation_report(report_data)
                        
                        # Descargar
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        filename = f"Validation_Report_{sensor_serial.replace(' ', '_')}_{timestamp}.html"
                        
                        st.success("‚úÖ Informe generado correctamente")
                        
                        st.download_button(
                            label="üíæ Descargar Informe HTML",
                            data=html_content,
                            file_name=filename,
                            mime="text/html",
                            use_container_width=True
                        )
                        
                    except Exception as e:
                        st.error(f"‚ùå Error al generar informe: {str(e)}")
                        with st.expander("üîç Ver detalles del error"):
                            import traceback
                            st.code(traceback.format_exc())


if __name__ == "__main__":
    main()